# HashTable


<!-- TOC -->

- [HashTable](#hashtable)
    - [Summary](#summary)
    - [设计思想](#设计思想)
        - [碰撞和处理](#碰撞和处理)
    - [相关定义](#相关定义)
        - [槽](#槽)
        - [散列函数](#散列函数)
        - [载荷因子](#载荷因子)
    - [散列函数实现](#散列函数实现)
        - [散列正整数](#散列正整数)
        - [散列浮点数](#散列浮点数)
        - [散列字符串](#散列字符串)
            - [一个简单的实现](#一个简单的实现)
            - [结合《算法导论》和《算法4》的方法](#结合算法导论和算法4的方法)
        - [折叠法](#折叠法)
        - [平方取中法](#平方取中法)
    - [处理冲突的两种方法简述](#处理冲突的两种方法简述)
    - [处理冲突——链接法](#处理冲突链接法)
        - [复杂度](#复杂度)
            - [查找元素](#查找元素)
            - [插入元素](#插入元素)
            - [删除元素](#删除元素)
    - [处理冲突——开放寻址法](#处理冲突开放寻址法)
        - [探查序列](#探查序列)
        - [插入逻辑](#插入逻辑)
        - [查找逻辑](#查找逻辑)
        - [删除逻辑](#删除逻辑)
        - [均匀散列](#均匀散列)
        - [线性探测（Linear Probing）方法](#线性探测linear-probing方法)
        - [平方探测（Quadratic probing，也称为二次探查）方法](#平方探测quadratic-probing也称为二次探查方法)
            - [系数选择](#系数选择)
        - [双重散列（Double hashing）方法](#双重散列double-hashing方法)
        - [开放寻址散列的分析](#开放寻址散列的分析)
    - [分析散列搜索算法](#分析散列搜索算法)
    - [References](#references)

<!-- /TOC -->


## Summary
1. A hash table is an effective data structure for implementing dictionaries. 
2. Although searching for an element in a hash table can take as long as searching for an element in a linked list — $O(n)$ time in the worst case — in practice, hashing performs extremely well. 
3. Under reasonable assumptions, the average time to search for an element in a hash table is $O(1)$.


## 设计思想
1. 数据在保存的时候分类的越高效，则搜索的时候也就越高效。
2. 除了类似于图书分类法这样分层的、抽象属性分类方法以外，还有一大类分类方法就是按照数值范围分类的，比如按照名次分班。
3. 例如按照 50 人一个班来分，当你想找上次考试排 128 名的人，就应该去 3 班找。但除非 3 班是按照名次排座位，否则你去了3班还是要在 50 个人里面一个一个找，而不能快速定位。
4. 那么进一步，假设 10 人一班，这样你可以快速先定位到 13 班，在 13 班里只需要在 10 个人里面一个一个找，会快很多。
5. 最极端的情况是 1 个人一班，那么你只要定位到 128 班，就立刻找到了那个人，你在班里不用再找了。
6. 这时看起来找班级变成了低效的事情，但实际的情况是，班级都是按照顺序依次排列的，所以比如一层 10 班，128 班就在 13 层。
7. 在计算机中只要是按照顺序，那么寻址的速度还是高效的。
8. 所以如果能把客观的学生都映射到唯一的数字名次上，那么想找到一个名次的人就会很高效。

### 碰撞和处理
1. 将学生映射为名次的话，显然就会出现成绩相同名次相同的情况。
2. 名次的情况，一般都是按照链接法来解决冲突，即并列第几名。把两个名次相同的学生排在一个班，这样找起来也是很方便的。
3. 当然也可以很变态的采用线性探测法，成绩一样的也名次递增，也是挨在一起的。
4. 归根结底都是让搜索时如果发现冲突可以很快的找到重复的元素。
5. 至于为了避免基础线性探测元素聚集而加入步长的情况，如果使用简单高效的步长计算方法，找到重复元素也不会明显的变慢。


## 相关定义
### 槽
1. 散列表中的每个位置通常被称为 **槽**（slot），其中可以存储一个元素。
2. 槽用一个从 0 开始的整数标记，例如 0 号槽、1 号槽、2 号槽，等等。
3. 初始情形下，散列表中没有元素，每个槽都是空的。

### 散列函数
1. **散列函数**（hash function）将散列表中的元素与其所属位置对应起来。对散列表中的任一元素，散列函数返回一个介于 `0` 和 `槽长度-1` 之间的整数。
2. 假设有一个由整数元素 54、26、93、17、77 和 31 构成的集合及一个 11 个槽的散列表。
3. 首先来看第一个散列函数，它有时被称作 “取余函数”，即用一个元素除以散列表的大小，并将得到的余数作为散列值（`h(item) = item%11`）。取余函数是一个很常见的散列函数，这是因为结果必须在槽编号范围内。
4. 计算出散列值后，就可以将每个元素插入到相应的位置。
5. 搜索目标元素时，仅需使用散列函数计算出该元素的槽编号，并查看对应的槽中是否有值。
6. 因为计算散列值并找到相应位置所需的时间是固定的，所以搜索操作的时间复杂度是 $O(1)$。如果一切正常，那么我们就已经找到了常数阶的搜索算法。
7. 但只有当每个元素的散列值不同时，这个技巧才有用。如果两个元素的散列值相同这就有问题了。
8. 散列函数确实会将两个元素都放入同一个槽，这种情况被称作 **冲突**（collision），也叫 “碰撞”。需要其他方法解决碰撞问题，但时间复杂度就会有所增加。

### 载荷因子
槽位占用率被称作 **载荷因子**，记作 $λ$。


## 散列函数实现
1. 给定一个元素集合，能将每个元素映射到不同的槽，这种散列函数称作 **完美散列函数**。
2. 如果元素已知，并且集合不变，那么构建完美散列函数是可能的。不幸的是，给定任意一个元素集合，没有系统化方法来保证散列函数是完美的。所幸，不完美的散列函数也能有不错的性能。
3. 构建完美散列函数的一个方法是增大散列表，使之能容纳每一个元素，这样就能保证每个元素都有属于自己的槽。这样的散列表就成为了一个 **直接寻址表**。
4. 当元素个数少时，这个方法是可行的，不过当元素很多时，就不可行了。如果元素是 9 位的社会保障号，这个方法需要大约 10 亿个槽。如果只想存储一个班上 25 名学生的数据，这样做就会浪费极大的内存空间。
5. 我们的目标是创建这样一个散列函数：冲突数最少，计算方便，元素均匀分布于散列表中。有多种常见的方法来扩展取余函数，下面介绍其中的几种。
6. 你也许能想到多种计算散列值的其他方法。重要的是，散列函数一定要高效，以免它成为存储和搜索过程的负担。如果散列函数过于复杂，计算槽编号的工作量可能比在进行顺序搜索或二分搜索时的更大，这可不是散列的初衷。

### 散列正整数
1. 将整数散列最常用方法是除留余数法。我们选择大小为质数 $M$ 的数组，对于任意正整数 $k$，计算 $k$ 除以 $M$ 的余数。
2. $M$ 要避免选择一些值，最常见的比如说 $k$ 的进制的幂。例如，如果键是十进制数而 $M$ 为 $10^k$，那么我们只能利用键的后 $k$ 位。例如 $M=100$，那么对于任何数，我们的散列都只是根据个位和十位来划分了，再高的数位不管是什么都不会有影响。
3. 通常会使用质数作为 $M$ 的值。如果 $k$ 是无规律的均匀分布的，那么是不是质数并没有关系，结果都是均匀的分布在 0 和 $M-1$ 之间。但是 $k$ 的值时常是有规律的，或者至少不是那么均匀的部分有规律的，那么质数的用处就体现出来了。例如下面的一组输入
    Input | Modulo 8 | Modulo 7
    --|--|--
    0  | 0 | 0
    4  | 4 | 4
    8  | 0 | 1
    12 | 4 | 5
    16 | 0 | 2
    20 | 4 | 6
    24 | 0 | 3
    28 | 4 | 0
4. 为什么 $M$ 为 8 时余数那么有规律，但 $M$ 为 7 时就没有那么有规律？因为这一组数是周期性的和 8 有关系，也就是周期性的是 8 的倍数或 8 的倍数加 4。
5. 然后我使用了下面一组数据
    Input | Modulo 9 | Modulo 7
    --|--|--
    0  | 0 | 0
    3  | 3 | 3
    6  | 6 | 6
    7  | 7 | 0
    10 | 1 | 3
    13 | 4 | 6
    14 | 5 | 0
    17 | 8 | 3
    20 | 2 | 6
6. 现在有意思了，变成质数时很有规律了。因为这一组数是周期性的和 7 有关。
7. 也就是说，之所以要使用质数，并不是质数本身更不容易引起冲突，而是我们一般情况下的数据和质数搭配起来更不容易引起冲突。例如我们的数据可能周期性的和 2 有关、和 10 有关之类的。
8. 结合上面两个例子，如果要冲突比较多，那么输入的数据要相对于 $M$ 有某种 “周期性”。也就是输入的数据每隔一小段它们的差距正好等于 $M$。（当然输入时并不一定就是按照从小到大的顺序，但如果按照从小到大顺序排列，就会看到这个周期性。）
9. 这个周期越短，冲突就会越多。在一个确定的 $M$ 下，周期最短的情况就是所有的输入都相差 $M$ 的整数倍，周期最长的情况呢？所有的输入组成公差为 1 的等差数列时周期会很长，如果是一些特殊的输入会不会更长呢？
10. 但还有一种情况下，周期也会是比较短的，那就是输入数据组成以 $M$ 的因数为公差的等差数列。比如 $M$ 有一个因数是 $q$，那以此为公差的输入只要 $M/q$ 次就能完成一个周期，进而散列出相同的结果。
11. 例如上面第一个表格中，$M$ 是 8，它有因数 4，而数据的公差正好就是 4，所以只要两次就能完成一个周期。
12. 数据中出现等差数列也是挺常见的，例如 $2^n$ 就很普遍。所以在不确定输入的情况下为了让这里的周期性尽量的长，就应该使用质数作为 $M$，避免 $M$ 的因数符合数据中的规律。

### 散列浮点数
1. 如果键是 0 到 1 之间的实数，我们可以将它乘以 $M$ 并四舍五入得到一个 0 至 $M-1$ 之间的索引值。
2. 尽管这个方法很容易理解，但它是有缺陷的，因为这种情况下键的高位起的作用更大，最低位对散列的结果没有影响。修正这个问题的办法是将键表示为二进制数然后再使用除留余数法。

### 散列字符串
#### 一个简单的实现
1. 我们也可以为基于字符的元素（比如字符串）创建散列函数。可以将单词 “cat” 看作序数值序列:
    ```sh
    >>> ord('c')
    99
    >>> ord('a')
    97
    >>> ord('t')
    116
    ```
    因此，可以将这些序数值相加，并采用取余法得到散列值
2. 为字符串构建简单的散列函数
    ```py
    def hash(astring, tablesize):
        sum = 0
        for pos in range(len(astring)):
            sum = sum + ord(astring[pos])

        return sum % tablesize
    ```
3. 有趣的是，针对同素异序词，这个散列函数总是得到相同的散列值。要弥补这一点，可以用字符位置作为权重因子
    ```py
    def hash(astring, tablesize):
        sum = 0
        for pos in range(len(astring)):
            sum = sum + ord(astring[pos]) * (pos+1)

        return sum % tablesize


    print(hash('cat', 11)) # 3
    print(hash('tac', 11)) # 2
    ```

#### 结合《算法导论》和《算法4》的方法
1. 下面是对 ASCII 字符串散列的函数，所以使用了基准值 128。这个基准值相当于上面的权重因子，但是不懂具体值的选择有什么讲究。TODO
2. 每次循环都是用之前的散列值乘以基准值，再加上新字符的值，然后求模并作为新的散列值
    ```cpp
    int hash_fn_str (char* str) {
        int hash = 0;
        char* ptr = str;
        while (*ptr != '\0') {
            hash = (128 * hash + *ptr++) % SIZE;
        }
        return hash;
    }
    ```
3. 这里与上面的方法不同，并不是最后计算完总和再求模，而是每一步都求模。至少有一个原因应该是防止溢出吧。
4. 以两个字符的字符串为例，看看基准值在处理同素异序词的效果。如果没有基准值，那么 `"AB"` 和 `"BA"` 散列计算如下
    ```
    mod( mod(A) + B )
    mod( mod(B) + A )
    ```
5. 第一行，`mod(A)` 在求模的结果还是 `mod(A)`，之后再加上 `mod(B)`；第二行同样，结果还是 `mod(A) + mod(B)`。
6. 即使基准值是加上去而不是乘上去，结果还是一样的。因为加法只是偏置而不是权重，不同位置的字符还是没有体现出不同来
    ```
    mod( mod(A) + 128 + B )
    mod( mod(B) + 128 + A )
    ```
    
### 折叠法
1. 折叠法先将元素切成等长的部分（最后一部分的长度可能不同），然后将这些部分相加，得到散列值。
2. 假设元素是电话号码 436-555-4601，以 2 位为一组进行切分，得到 43、65、55、46 和 01。将这些数字相加后，得到 210。假设散列表有 11 个槽，接着需要用 210 除以 11，并保留余数 1。所以，电话号码 436-555-4601 被映射到散列表中的 1 号槽。
3. 有些折叠法更进一步，在加总前每隔一个数反转一次。就本例而言，反转后的结果是：43+56+55+64+01=219，219%11=10。

### 平方取中法
1. 另一个构建散列函数的数学技巧是平方取中法：先将元素取平方，然后提取中间几位数。
2. 如果元素是 44，先计算 44^2=1936，然后提取中间两位 93，继续进行取余的步骤，得到 5（93%11）。


## 处理冲突的两种方法简述
1. 当两个元素被分到同一个槽中时，必须通过一种系统化方法在散列表中安置第二个元素。这个过程被称为处理冲突。
2. 前文说过，如果散列函数是完美的，冲突就永远不会发生。然而，这个前提往往不成立，因此处理冲突是散列计算的重点。
3. 处理冲突有两类方法：**链接法**（chaining） 和 **开放寻址法**（open addressing）。
4. 链接法会把散列值为同一个槽位的元素放在当前槽位的链表里；而开放寻址法不会在一个槽位存储超过一个元素，如果一个元素散列值和之前的重复，会进行再散列找到新的槽位。
5. 因此在开放寻址法中，散列表可能被填满而不能再插入元素。同理，开放寻址法的装载因子 $α$ 也不会大于 1。
6. The advantage of open addressing is that it avoids pointers altogether. Instead of following pointers, we compute the sequence of slots to be examined. The extra memory freed by not storing pointers provides the hash table with a larger number of slots for the same amount of memory, potentially yielding fewer collisions and faster retrieval.


## 处理冲突——链接法
1. 链接法处理冲突的方式是让每个槽有一个指向元素集合（比如链表）的引用，散列到同一个位置上的元素都会被存入到该位置对应的集合中。
2. 搜索目标元素时，我们用散列函数算出它对应的槽编号。由于每个槽都有一个元素集合，因此需要再搜索一次，才能得知目标元素是否存在。
3. 搜索、插入操作是通过元素的 key 来计算槽位，并在对应的元素集合里进行；而删除则有所不同：
    * 如果是元素集合是单向链表，即使知道该元素，也不能知道该元素的 prev，所以还是要通过元素的 key 进行散列找到单向列表的 head，然后再一步步找到被删除元素的 prev；
    * 如果元素集合是双向链表，则删除时可以直接通过元素来找到它的 prev 和 next 进行删除；不过如果删除的是 head，那么还是要哈希来找到槽位出的指针来修改指向。
4. 实现在 `./SeparateChaining.c`。

### 复杂度
1. 给定一个存放 $n$ 个元素、拥有 $m$ 个槽位的散列表 $T$，定义 $T$ 的 **装载因子**(load factor) $α$ 为 $n/m$, 也就是每个链表的平均存储元素数。
2. 我们的分析将基于 $α$ 值，它可能小于、等于或大于 1。
3. 链接法最坏的情况是 $O(n)$ 级别的，也就是 $n$ 个元素都放到了同一个链表中。
4. 平均的情况依赖于所选取的散列函数的性能，也就是该函数能在多大程度上将 $n$ 个元素均匀的分布在 $m$ 个槽位中。
5. 这里先假设任何一个给定元素等可能的散列到 $m$ 槽位中的任何一个，且与其他元素被散列到什么位置无关。我们称这个假设为 **简单均匀散列**(simple uniform hashing)。

#### 查找元素
1. 既然已经假设是简单均匀散列，那么也就是说每个链表的元素数是 $α$（$n/m$）。
2. 分为两种情况来考虑：查找的元素不在散列表中（查找失败），查找的元素在散列表中（查找成功）。
3. 另外，不管失败还是成功，都要先计算一次散列值，这个计算期望是常数时间。
4. 因此，查找失败时会遍历整个链表，再加上之前计算散列值，因此时间复杂度为 $Θ(α+1)$；查找成功时，根据《算法导论》146页上没看懂的推导，时间复杂度仍然为 $Θ(α+1)$。不懂为什么不是 $Θ(α/2 + 1)$ 之类的，查找到的元素等可能的位于链表的任何位置。
5. 总之，查找成功或者失败，都是 $Θ(α+1)$。而槽位数量 $m$ 和元素数量 $n$ 成正比，所以 $α$ 为常数，因此以大 $O$ 来表示，时间复杂度就是 $O(1)$。

#### 插入元素
插入操作把元素插入到当前槽位链表的头部，所以时间是 $O(1)$。

#### 删除元素
不管是双向链表还是单向链表，根据删除的算法实现，时间复杂度仍然是和查找一样， $O(1)$。


## 处理冲突——开放寻址法
### 探查序列
1. To perform insertion using open addressing, we successively examine, or **probe**, the hash table until we find an empty slot in which to put the key.
2. Instead of being fixed in the order $0, 1,..., m-1$ (which requires $Θ(n)$ search time), the sequence of positions probed depends upon the key being inserted. 
3. To determine which slots to probe, we extend the hash function to include the probe number (starting from $0$) as a second input. With open addressing, we require that for every key $k$, the **probe sequence** is $\langle h(k, 0), h(k, 1),..., h(k, m-1) \rangle$. 也就是说，散列函数的初次散列和之后的在散列，会按照这个序列计算出的位置依次遍历槽位。
4. 再散列可以很简单的尝试后一个位置，也可以是其他复杂一些的再散列逻辑，但都要保证再散列可以遍历所有的可插入槽位。
5. 不用链接法中的指针和链表，节省下来的空间可以用于提供更多的槽位，潜在的减少了冲突，提高了检索速度。

### 插入逻辑
1. 对于待插入的关键字 $k$，散列函数会依次计算出探查序列中的位置，直到找到一个可插入槽位或者遍历完所有槽位。
2. 考虑到下面讲到了删除逻辑，如果哈希表是支持删除功能的，那么这里的 “可插入槽位” 就包括从来没有插入过的槽位和曾经插入过但是被删除的槽位。

### 查找逻辑
1. 查找逻辑和插入逻辑差不多。先根据当前 key 计算散列值，依次查看探查序列中的槽位，如果直到找到空槽位或者找完了所有槽位还没找到，就说明要查找的元素不存在。
2. 注意，这里说的 “空槽位”，是指它从来没有被插入元素一直为空的，而不包括曾经插入但之后删除了。因为要查找的元素可能是在这个空槽为曾经不空的时候插入到了它所在的探查序列的后方某个槽位。
3. 插入元素时我们会不断的再散列找到一个没有被占用的槽位，假如我们的散列表没有删除操作，那么查找时如果一直再散列到一个没有被占用的槽位时还没有找到对应 key 的元素，那就说该元素不存在。
4. 但是我们要实现删除操作，那么比如 `k` 元素插入时散列值为 `i`，但是 `i` 被占用了，所以 `k` 元素放到了 `j`。之后 `i` 所在的元素被删除了，我们想要查找 `k` 元素时散列到 `i`，发现没有被占用，但现在就不能说 `k` 元素不存在。
5. 所以我们必须要区分一直为空和被删除了的两种状态。查找时再散列遇到一直为空就说明不存在，遇到被删除的则要继续再散列。

### 删除逻辑
1. 也是散列和再散列的过程，只不过删除之后不能把槽位标记为空，而要标记为被删除。
2. When we use the special value `DELETED`, however, search times no longer depend on the load factor $O(α)$, and for this reason chaining is more commonly selected as a collision resolution technique when keys must be deleted. 被删除的元素不会计入 $α$ 的计算，但是又会影响查找。
3. 因此，在必须要删除关键字的应用中，更常见的做法是采用链接法来解决冲突。

### 均匀散列
1. 对于一个拥有 $m$ 个槽位的哈希表，为了插入一个元素，它需要探查的可能性有多少种？
2. 第一次散列有 $m$ 中可能性，如果第一次没找到空位，再散列有 $m-1$ 种可能性，如果还没中，再散列有 $m-2$ 种可能性。以此类推，一个元素探查的可能性一共有 $m!$ 种。
3. 如果一个探查方法可以实现这么多种可能性，那么称其为 **均匀散列**（uniform hashing）。但是真正的均匀很难做到，实际应用中常常采用它的一些近似方法。

### 线性探测（Linear Probing）方法
1. 再散列函数是线性方程（一次方程），也就是说探查序列为：$(初始散列值 + C_1 * i) \mod m$。$C_1$ 是常数，$i=0, 1, ..., m-1$。
2. 最基础的线性探测就是 $C_1 = 1$ 时的情况，也就是先用散列函数计算出初始散列值 `x`，如果 `x` 的槽位已经被占用那就尝试 `x+1`，以此类推。
3. 为了遍历散列表，可能需要往回检查第一个槽。也就是说一直找到散列表最后一个仍然没有空位，则循环到头部再检测。
4. 实现在 `./LinearProbing.c`。
5. 以 1 为跨步（skip）的线性探测有个缺点，那就是会使散列表中的元素出现聚集现象，称为 **一次群集**（primary clustering）。也就是说，如果一个槽发生太多冲突，线性探测会填满其附近的槽，而这会影响到后续插入的元素，有时要越过数个槽位才能找到一个空槽。
6. 要避免元素聚集，一种方法是扩展线性探测，不再依次顺序查找空槽，而是跳过一些槽，这样做能使引起冲突的元素分布得更均匀。例如采用 $C_1 = 3$ 探测策略处理冲突后时，如果发生冲突时，为了找到空槽，该策略每次跳两个槽。
7. 注意，跨步的大小要能保证表中所有的槽最终都被访问到，否则就会浪费槽资源。要保证这一点，常常建议散列表的大小为质数。互质也是可以的，例如散列表大小为 10，skip 为 3。上面基础的线性探测的跨步值为 1，而 1 就是和任何数互质的。TODO 质数原理。
8. 线性探测第一次散列有 $m$ 种可能性，因为之后再散列规则都是确定的加上同样的常数，所以整体的探查可能性只有 $m$。

### 平方探测（Quadratic probing，也称为二次探查）方法
1. 再散列函数是二次方程，也就是说探查序列为：$(初始散列值 + C_1 * i^2 + C_2 * i) \mod m$。
2. 一个简单的实现是 $C_1 = 1$、$C_2 = 0$：如果第一个散列值是 $h$，后续的散列值就是 $h+1$、$h+4$、$h+9$、$h+16$，等等。换句话说，跨步大小是一系列完全平方数。
3. TODO。《算法导论》思考题 11-3 提出怎样选择 $C_1$ 和 $C_2$ 的值来保证可以散列到所有插槽，没看懂，不过下面系数选择里提供了几种可用方案。
4. 线性探测里，即使参数 $C_1$ 不等于 1，但每次的跨步也是固定的。也就是说相近的值在经过若干次再散列后仍然是相近的。
5. 而平方探测因为跨度逐渐变大，所以相近的值在少数几次再散列后位置就会明显拉大，所以不会出现很明显的集群。但是仍然，如果两个关键字的初始探查位置相同，它们的探查序列也相同，所以仍然会导致轻度的群集，称为 **二次群集**（secondary clustering）。
6. 实现在 `./QuadraticProbing.c`。
7. 虽然这里再散列改成了完全平方数，但因为也是确定的常数，所以探查可能性仍然是 $m$ 种。

#### 系数选择
1. 第一种方案很简单，就是上面的简单实现，跨步是一系列连续的完全平方数；同时保证散列表的槽位数是质数即可。
2. 第二种方案是让跨步为 $(i^2 + i) / 2, \enspace i=0, 1, ..., m-1$，（相当于 $C_1 = C_2 = 1/2$）同时让散列表槽位数是 2 的幂。TODO，也不互质。槽位数定为 2 的幂，在载荷到达阈值的时候扩建哈希表时可以直接很简单的翻倍，保证了槽位数仍然是 2 的幂。
3. 第三种方案是让跨步为 $(-1)^i * i^2, i=0, \enspace 1, ..., m-1$，同时让散列表槽位数对 4 求模的结果为 3。TODO


### 双重散列（Double hashing）方法
1. 探查序列为：$(初始散列值 + i * h_1(k)) \mod m$，$h_1$ 是另一个散列函数。
2. Double hashing offers one of the best methods available for open addressing because the permutations produced have many of the characteristics of randomly chosen permutations. 
3. 其实就是把线性探查再散列的 $C_1$ 从常数变成了函数 $h_1$。也就是再散列时加上的数不是提前确定的，而是在运行时根据参数 $key$ 通过函数 $h_1$ 计算才确定的。
4. 线性探测中说到，skip 要和 $m$ 互质才能保证遍历所有的槽位，那么这里就要求 $h_1$ 返回的数要和 m 互质。有两种简便方法可以实现这样的互质：
    * m 取 2 的幂，$h_1$ 返回一个奇数。
    * m 取质数，$h_1$ 返回一个比 $m$ 小的正整数。TODO，那么 $h_1$ 要返回比 $m$ 小的？
5. 例如，可以让 $h_1$ 返回 $1 + (k \mod (m-1))$。假如 $k$ 为 123456，$m$ 为 701。初始散列值（假设初始散列是直接对 $m$ 求模）的结果为 80，$h_1$ 同样要根据当前的 $k$ 求值，返回 257。因此每次再散列都是在前一次的基础上加 257，即第一次再散列结果为 337，第二次再散列结果为 594…… 
6. 初始散列会产生 $m$ 种可能性，再散列时 $h_1$ 也会产生 $m$ 种可能性，所以双重散列方法探查的可能性是 $m^2$ 种，非常接近理想的均匀散列的性能。
7. 实现在 `DoubleHashing.c`。

### 开放寻址散列的分析
1. 给定一个装载因子为 $α$ 的开放寻址散列表，并假设是均匀散列的。
2. 对于一次失败的查找，期望的探查次数至多为 $1/(1-α)$。证明 TODO。
3. 平均情况下，插入一个元素之多需要 $1/(1-α)$ 探查。因为要插入就要先探查到一个空槽，这就相当于一次失败的查找。
4. 对于一次成功的查找，探查次数至多为 $\frac{1}{α} \ln\frac{1}{1-α}$。证明 TODO。


## 分析散列搜索算法
1. 在最好情况下，散列搜索算法的时间复杂度是 O(1)。然而，因为可能发生冲突，所以比较次数通常不会这么简单。
2. 在分析散列表的使用情况时，最重要的信息就是载荷因子 $α$。
3. 从概念上来说，如果 $α$ 很小，那么发生冲突的概率就很小，元素也就很有可能各就各位。如果 $α$ 很大，则意味着散列表很拥挤，发生冲突的概率也就很大。
4. 因此，冲突解决起来会更难，找到空槽所需的比较次数会更多。一个不错的经验规则是：一旦 $α$ 大于 0.7，就调整散列表的长度。
5. 若采用链接法，冲突越多，每条链上的元素也越多。
6. 和之前一样，来看看搜索成功和搜索失败的情况 TODO
    <img src="./images/01.png" width="400" style="display: block; margin: 5px 0 10px;" />


## References
* [《Python数据结构与算法分析（第2版）》](https://book.douban.com/subject/34785178/)
* [《算法图解》](https://book.douban.com/subject/26979890/)
* [算法导论](https://book.douban.com/subject/20432061/)
* [算法（第4版）](https://book.douban.com/subject/19952400/)
* [stackoverflow](https://stackoverflow.com/a/3613423)