# Basic


## 从数据中学习
1. 神经网络的特征就是可以从数据中学习。
2. 所谓“从数据中学习”，是指可以由数据自动决定权重参数的值。在实际的神经网络中，参数的数量成千上万，在层数更深的深度学习中，参数的数量甚至可以上亿，想要人工决定这些参数的值是不可能的。
3. 对于线性可分问题，感知机是可以利用数据自动学习的。根据“感知机收敛定理”，通过有限次数的学习，线性可分问题是可解的。但是，非线性可分问题则无法通过（自动）学习来解决。

### 数据驱动

### 训练数据和测试数据
1. 机器学习中，一般将数据分为训练数据（也被称为监督数据）和测试数据两部分来进行学习和实验等。
2. 首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。
3. 为什么需要将数据分为训练数据和测试数据呢？因为我们追求的是模型的泛化能力。为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。
4. 泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据，即，从适用于训练数据泛化到适用于实际数据）的能力。获得泛化能力是机器学习的最终目标。
5. 因此，仅仅用一个数据集去学习和评价参数，是无法进行正确评价的。这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况。顺便说一下，只对某个数据集过度拟合的状态称为过拟合（over fitting）。避免过拟合也是机器学习的一个重要课题。


## 损失函数
1. 神经网络的学习通过某个指标表示现在的状态。然后，以这个指标为基准，寻找最优权重参数。
2. 神经网络的学习中所用的指标称为损失函数（loss function）。这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等。
3. 损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。以“性能的恶劣程度”为指标可能会使人感到不太自然，但是如果给损失函数乘上一个负值，就可以解释为“在多大程度上不坏”，即“性能有多好”。

### 均方误差（mean squared error）损失函数
<img src="./images/01.gif" width="300" style="display: block; background: white;" />

1. 这里，`y` 表示神经网络的输出，`t` 表示监督数据，`k` 表示数据的维数。
2. 在识别一个手写数字的例子中，`y`是对该数字判断是从 0 到 9 的概率，`t`是对应的监督数据 one-hot 表示
   ```py
    y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
    t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
    ``` 
3. 上述数据可以看出，神经网络判断该图片最后可能是数字 2，有60%的概率；监督数据也表示该图片就是数字 2。
4. 根据均方误差函数的逻辑可以看出，当神经网络对一条数据判断的越准确是，y 和 t 就越相似，均方误差函数的值也就最小，损失也就最小，就代表判断越准确。
5. 该损失函数的 Python 实现如下
    ```py
    def mean_squared_error(y, t):
        return 0.5 * np.sum((y-t)**2)
    ```
6. 测试两次神经网络输出的损失函数计算
    ```py
    import numpy as np

    def mean_squared_error(y, t):
        return 0.5 * np.sum((y-t)**2)  
            
    # 假设“2”为正确解
    t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

    # 例1：判断“2”的概率最高的情况（0.6）
    y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]

    print(mean_squared_error(np.array(y), np.array(t)))
    # 0.09750000000000003

    # 例2：判断“7”的概率最高的情况（0.6）
    y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
    print(mean_squared_error(np.array(y), np.array(t)))
    # 0.5975
    ```
    可以看到，根据损失函数的计算，可以的出第一种情况的损失要明显比第二种少，也就是说准确的概率更大。

### 交叉熵误差（cross entropy error）损失函数
<img src="./images/02.gif" width="300" style="display: block; background: white;" />

1. log 表示以e为底数的自然对数。
2. t 中只有正确解标签的索引为 1，其他均为 0（one-hot 表示）。因此，交叉熵误差实际上只计算对应正确解标签的输出的自然对数。
3. 神经网络对正确的选项给出的概率越高（越接近1），对数的值（负值）就越接近0，损失函数的结果作为该负值的绝对值也就越小。
4. Python 实现如下
    ```py
    def cross_entropy_error(y, t):
        delta = 1e-7
        return -np.sum(t * np.log(y + delta))
    ```
5. 函数内部在计算 `np.log` 时，加上了一个微小值 `delta`。这是因为，当出现 `np.log(0)` 时，`np.log(0)` 会变为负无限大的 `-inf`，这样一来就会导致后续计算无法进行。作为保护性对策，添加一个微小值可以防止负无限大的发生。
6. 测试
    ```py
    import numpy as np


    def cross_entropy_error(y, t):
        delta = 1e-7
        return -np.sum(t * np.log(y + delta))
            
    # 假设“2”为正确解
    t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

    # 例1：判断“2”的概率最高的情况（0.6）
    y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]

    print(cross_entropy_error(np.array(y), np.array(t)))
    # 0.510825457099338

    # 例2：判断“7”的概率最高的情况（0.6）
    y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
    print(cross_entropy_error(np.array(y), np.array(t)))
    # 2.302584092994546
    ```


## mini-batch 学习
1. 上面的例子都是对单一输出进行损失函数计算，如果一次对多个输出结果进行计算，可以写成下面的形式
    <img src="./images/03.gif" width="300" style="display: block; background: white;" />
    也就是计算 n 个数据计算损失函数结果的平均值
2. 训练数据集的数据通常量很大，不可能使用所有的数据进行训练。神经网络的学习也是从训练数据中选出一批数据（称为 mini-batch, 小批量），然后对每个 mini-batch 进行学习。比如，从 60000 个训练数据中随机选择 100 笔，再用这 100 笔数据进行学习。这种学习方式称为 mini-batch 学习。

### 从数据集中选择批量数据
```py
(x_train, t_train), (x_test, t_test) = \
    load_mnist(normalize=True, one_hot_label=True)

# 训练数据的所有的输入数据和监督数据
# 60000 条输入数据，每条数据是 28 × 28 像素的图像数据
# 60000 条监督数据，每条数据是一个 1 九个 0 的一维数组
print(x_train.shape) # (60000, 784) 
print(t_train.shape) # (60000, 10)

train_size = x_train.shape[0]
batch_size = 10
# 从 60000 （train_size）条训练数据中随机选择 10 （batch_size）条数据作为 mini-batch 进行训练
batch_mask = np.random.choice(train_size, batch_size) # batch_size 个索引号
x_batch = x_train[batch_mask] # batch_size 个随机输入数据
t_batch = t_train[batch_mask] # 对应的 batch_size 个监督数据
```

### mini-batch 版交叉熵误差的实现
1. 可以同时处理单个数据和批量数据（数据作为 batch 集中输入）两种情况的函数
    ```py
    # y 是神经网络的输出，t 是监督数据
    def cross_entropy_error(y, t):
        # 如果输入的是单条数据（一维数组），则格式化为二维数组，以便下面的统一计算
        if y.ndim == 1:
            t = t.reshape(1, t.size)
            y = y.reshape(1, y.size)
        # 计算所有数据的平均交叉熵误差
        batch_size = y.shape[0]
        return -np.sum(t * np.log(y + 1e-7)) / batch_size
    ```
2. 当监督数据是标签形式（非 one-hot 表示，而是像“2”“7”这样的标签）时，交叉熵误差可通过如下代码实现
    ```py
    def cross_entropy_error(y, t):
        if y.ndim == 1:
            t = t.reshape(1, t.size)
            y = y.reshape(1, y.size)

        batch_size = y.shape[0]
        # np.arange (batch_size) 会生成一个从 0 到 batch_size-1 的数组。
        # 因为不是 one-hot 形式，所以 t 的每一项不再是一个十项数组，而只是一个表示正确答案的从 0 到 9 的整数。
        # 因此，y[np.arange(batch_size), t] 会抽出批处理输入中的每一个输出（一个10项数组）中正确结果对应的那
        # 个概率值。
        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
    ```


## 为何要设定损失函数
1. 上面我们讨论了损失函数，可能有人要问：“为什么要导入损失函数呢？”
2. 以数字识别任务为例，我们想获得的是能提高识别精度的参数，特意再导入一个损失函数不是有些重复劳动吗？也就是说，既然我们的目标是获得使识别精度尽可能高的神经网络，那不是应该把识别精度作为指标吗？
3. 假设某个神经网络正确识别出了 100 笔训练数据中的 32 笔，此时识别精度为 32% 。如果以识别精度为指标，即使稍微改变权重参数的值，识别精度也仍将保持在 32% ，不会出现变化。
4. 也就是说，仅仅微调参数，是无法改善识别精度的。即便识别精度有所改善，它的值也不会像 32.0123 ... % 这样连续变化，而是变为 33%、34% 这样的不连续的、离散的值。
5. 而如果把损失函数作为指标，则当前损失函数的值可以表示为 0.92543 ... 这样的值。并且，如果稍微改变一下参数的值，对应的损失函数也会像 0.93432 ... 这样发生连续性的变化。
6. 出于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。阶跃函数的导数在绝大多数地方（除了 0 以外的地方）均为 0。也就是说，如果使用了阶跃函数，那么即便将损失函数作为指标，参数的微小变化也会被阶跃函数抹杀，导致损失函数的值不会产生任何变化。